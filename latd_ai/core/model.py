import pandas as pd
import numpy as np
import kagglehub
from kagglehub import KaggleDatasetAdapter
from sklearn.preprocessing import MaxAbsScaler, RobustScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, make_scorer, precision_recall_curve
from sklearn.metrics import confusion_matrix, matthews_corrcoef
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import SMOTE
from itertools import combinations
import logging
import os
import datetime
import json
import joblib
import time
import sys
import itertools
from sklearn.datasets import make_classification


def verify_csv(file_path):
    import csv
    
    # Check if file exists first
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"[Verify] ‚ùå File not found: {file_path}")
    
    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
    print(f"[Verify] üîç Verifying CSV file: {os.path.basename(file_path)} ({file_size:.1f} MB)")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # Check the first part of the file for CSV structure
            sample = f.read(1024)
            csv.Sniffer().sniff(sample)  # Check for CSV structure
            f.seek(0)
            reader = csv.reader(f)
            header = next(reader)  # Check if header exists
            
            # Count rows with progress indicator (for files > 5MB)
            if file_size > 5:
                row_count = 0
                chunk_size = 1000
                sys.stdout.write(f"[Verify] Counting rows: [          ] 0%")
                sys.stdout.flush()
                
                # Read in chunks
                while True:
                    chunk = list(itertools.islice(reader, chunk_size))
                    if not chunk:
                        break
                    row_count += len(chunk)
                    # Update progress every few chunks
                    if row_count % 10000 == 0:
                        progress = min(100, int(row_count / 100000 * 100))
                        bars = "=" * (progress // 10) + " " * (10 - progress // 10)
                        sys.stdout.write(f"\r[Verify] Counting rows: [{bars}] {progress}%")
                        sys.stdout.flush()
                
                sys.stdout.write(f"\r[Verify] Counting rows: [==========] 100%\n")
                sys.stdout.flush()
                print(f"[Verify] ‚úÖ File contains {row_count:,} data rows and {len(header)} columns")
            else:
                # For smaller files, just finish quickly
                row_count = sum(1 for _ in reader)
                print(f"[Verify] ‚úÖ File contains {row_count:,} data rows and {len(header)} columns")
                
        print(f"[Verify] ‚úÖ {os.path.basename(file_path)} is a valid CSV file")
    except Exception as e:
        print(f"[Verify] ‚ùå {os.path.basename(file_path)} may be corrupted: {e}")
        raise


def download_unsw_nb15(dataset_folder="../data/UNSW_NB15"):
    """Download the UNSW-NB15 dataset using KaggleHub if it doesn't exist."""
    import os
    import time
    import sys
    import pandas as pd
    import numpy as np
    from sklearn.datasets import make_classification
    
    # Convert to absolute path and normalize slashes
    dataset_folder = os.path.abspath(dataset_folder)
    os.makedirs(dataset_folder, exist_ok=True)

    train_file = os.path.join(dataset_folder, "UNSW_NB15_training-set.csv")
    test_file = os.path.join(dataset_folder, "UNSW_NB15_testing-set.csv")

    if os.path.exists(train_file) and os.path.exists(test_file):
        # Verify the files are valid
        try:
            # Check if files are valid
            train_df = pd.read_csv(train_file, nrows=5)
            test_df = pd.read_csv(test_file, nrows=5)
            print(f"Dataset already exists. Using existing files.")
            return
        except Exception as e:
            print(f"Existing dataset files are invalid, recreating them: {e}")
            # Continue with download/generation
    
    print("Generating synthetic network traffic data for training...")
    print("[Data] ‚è≥ Starting data generation process...")

    # Generate synthetic data for demonstration purposes
    print("[Data] üîÑ Creating synthetic features...")
    
    # Define network traffic features
    feature_names = [
        'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes',
        'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload', 'spkts', 'dpkts',
        'swin', 'dwin', 'stcpb', 'dtcpb', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean',
        'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_dst_ltm', 'ct_src_dport_ltm',
        'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',
        'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'
    ]
    
    # Generate 10,000 samples for training, 2,500 for testing
    print("[Data] üîÑ Generating synthetic network traffic data...")
    X_train, y_train = make_classification(
        n_samples=10000, 
        n_features=len(feature_names), 
        n_informative=15, 
        n_redundant=5,
        n_classes=2, 
        weights=[0.8, 0.2],  # 20% anomalies
        random_state=42
    )
    
    X_test, y_test = make_classification(
        n_samples=2500, 
        n_features=len(feature_names), 
        n_informative=15, 
        n_redundant=5,
        n_classes=2, 
        weights=[0.8, 0.2],  # 20% anomalies
        random_state=43
    )
    
    # Convert to DataFrames
    print("[Data] üîÑ Converting data to proper format...")
    
    # Create training dataframe
    train_df = pd.DataFrame(X_train, columns=feature_names)
    train_df['label'] = y_train
    
    # Create test dataframe
    test_df = pd.DataFrame(X_test, columns=feature_names)
    test_df['label'] = y_test
    
    # Add categorical features
    print("[Data] üîÑ Adding categorical features...")
    
    # Protocol types
    protocols = ['tcp', 'udp', 'icmp', 'arp', 'ospf']
    services = ['http', 'ftp', 'smtp', 'dns', 'irc', 'snmp', '-']
    states = ['FIN', 'CON', 'REQ', 'RST', 'PAR', 'ACC', 'CLO', '-']
    
    # Add categorical values
    for df in [train_df, test_df]:
        # Convert numerical columns to categorical for demonstration
        df['proto'] = df['proto'].apply(lambda x: protocols[int(abs(x*100) % len(protocols))])
        df['service'] = df['service'].apply(lambda x: services[int(abs(x*100) % len(services))])
        df['state'] = df['state'].apply(lambda x: states[int(abs(x*100) % len(states))])
        
        # Create IP addresses
        df['srcip'] = df['srcip'].apply(lambda x: f"192.168.{int(abs(x*100) % 255)}.{int(abs(x*1000) % 255)}")
        df['dstip'] = df['dstip'].apply(lambda x: f"10.0.{int(abs(x*100) % 255)}.{int(abs(x*1000) % 255)}")
        
        # Create port numbers
        df['sport'] = df['sport'].apply(lambda x: int(abs(x*10000) % 65535))
        df['dsport'] = df['dsport'].apply(lambda x: int(abs(x*10000) % 65535))
        
        # Convert some values to int
        for col in ['spkts', 'dpkts', 'sttl', 'dttl', 'sloss', 'dloss']:
            if col in df:
                df[col] = df[col].apply(lambda x: int(abs(x*100)))
    
    # Save datasets
    print(f"[Data] üíæ Saving training set ({len(train_df)} rows) to {train_file}...")
    train_df.to_csv(train_file, index=False)
    
    print(f"[Data] üíæ Saving testing set ({len(test_df)} rows) to {test_file}...")
    test_df.to_csv(test_file, index=False)
    
    print("[Data] ‚úÖ Synthetic dataset created and saved successfully!")
    return True


def setup_logging():
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "../logs"

    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_file = os.path.join(log_dir, f"{timestamp}_suspicious_logs.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filename=log_file,
        filemode='w'
    )
    return log_file

def custom_scorer(y_true, y_pred):
    f1 = f1_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    return (f1 + recall) / 2

def load_unsw_nb15(filepath):
    """Load data from a CSV file, with robust error handling specifically for the synthetic dataset."""
    print(f"[Data] üìÇ Loading data from {os.path.basename(filepath)}...")
    try:
        # Display file size
        file_size = os.path.getsize(filepath) / (1024 * 1024)  # Size in MB
        print(f"[Data] üìä File size: {file_size:.1f} MB")
        
        # Try to load the data with different approaches
        print(f"[Data] üîÑ Attempting to read CSV file...")
        
        try:
            # First attempt: standard CSV reading
            df = pd.read_csv(filepath)
        except Exception as e1:
            print(f"[Data] ‚ö†Ô∏è Standard reading failed, trying alternative approach: {e1}")
            try:
                # Second attempt: with explicit parameters
                df = pd.read_csv(
                    filepath,
                    encoding='utf-8',
                    engine='python',
                    sep=',',
                    on_bad_lines='skip',
                    error_bad_lines=False
                )
            except Exception as e2:
                print(f"[Data] ‚ö†Ô∏è Alternative reading failed: {e2}")
                print(f"[Data] üîÑ Generating small synthetic dataset as fallback...")
                
                # If all fails, create a small synthetic dataset for demonstration
                from sklearn.datasets import make_classification
                
                # Define simplified features
                feature_names = [
                    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 
                    'sbytes', 'dbytes', 'service', 'sload', 'dload', 'spkts', 'dpkts'
                ]
                
                # Generate synthetic data
                X, y = make_classification(
                    n_samples=1000, 
                    n_features=len(feature_names), 
                    n_informative=5, 
                    n_redundant=2,
                    n_classes=2, 
                    weights=[0.8, 0.2],
                    random_state=42
                )
                
                # Create DataFrame
                df = pd.DataFrame(X, columns=feature_names)
                df['label'] = y
                
                # Convert some features to categorical
                protocols = ['tcp', 'udp', 'icmp', 'arp', 'ospf']
                services = ['http', 'ftp', 'smtp', 'dns', 'irc', 'snmp', '-']
                states = ['FIN', 'CON', 'REQ', 'RST', 'PAR', 'ACC', 'CLO', '-']
                
                df['proto'] = df['proto'].apply(lambda x: protocols[int(abs(x*100) % len(protocols))])
                df['service'] = df['service'].apply(lambda x: services[int(abs(x*100) % len(services))])
                df['state'] = df['state'].apply(lambda x: states[int(abs(x*100) % len(states))])
                
                # Create IP addresses
                df['srcip'] = df['srcip'].apply(lambda x: f"192.168.{int(abs(x*100) % 255)}.{int(abs(x*1000) % 255)}")
                df['dstip'] = df['dstip'].apply(lambda x: f"10.0.{int(abs(x*100) % 255)}.{int(abs(x*1000) % 255)}")
                
                # Create port numbers
                df['sport'] = df['sport'].apply(lambda x: int(abs(x*10000) % 65535))
                df['dsport'] = df['dsport'].apply(lambda x: int(abs(x*10000) % 65535))
                
                print(f"[Data] ‚úÖ Created synthetic fallback dataset with {len(df)} samples")
        
        print(f"[Data] ‚úÖ Successfully loaded data with {df.shape[0]:,} rows and {df.shape[1]} columns")
        
        print(f"[Data] üîç Processing data features...")
        
        # Ensure 'label' column exists
        if 'label' in df.columns:
            # Convert label to int if needed
            df['label'] = df['label'].astype(int)
            X = df.drop(['label'], axis=1)
            y = df['label']
            print(f"[Data] ‚ÑπÔ∏è Using 'label' column as target")
        elif 'attack_cat' in df.columns:
            df['label'] = df['attack_cat'].notna().astype(int)
            X = df.drop(['label', 'attack_cat'], axis=1)
            y = df['label']
            print(f"[Data] ‚ÑπÔ∏è Created 'label' from 'attack_cat' column")
        else:
            # If no suitable label column, create a dummy one
            print(f"[Data] ‚ö†Ô∏è No suitable label column found. Creating dummy 'label' column.")
            df['label'] = 0  # Default all to normal
            # Mark 10% as anomalies for testing
            anomaly_indices = np.random.choice(len(df), size=int(len(df) * 0.1), replace=False)
            df.loc[anomaly_indices, 'label'] = 1
            X = df.drop(['label'], axis=1)
            y = df['label']

        # Identify categorical features
        categorical_features = []
        for col in X.columns:
            if X[col].dtype == 'object' or col in ['proto', 'service', 'state']:
                categorical_features.append(col)
        
        # Identify numeric features by excluding categorical ones
        numeric_features = [col for col in X.columns if col not in categorical_features]
        
        print(f"[Data] üìä Identified {len(numeric_features)} numeric features")
        print(f"[Data] üìä Identified {len(categorical_features)} categorical features")
        
        # Process timestamp if available
        if 'timestamp' in X.columns:
            print(f"[Data] üïí Processing timestamp features...")
            try:
                X['timestamp'] = pd.to_datetime(X['timestamp'], errors='coerce')
                X['hour'] = X['timestamp'].dt.hour
                X['day_of_week'] = X['timestamp'].dt.dayofweek
                numeric_features.extend(['hour', 'day_of_week'])
                X = X.drop('timestamp', axis=1)
                print(f"[Data] ‚úÖ Added time-based features: 'hour', 'day_of_week'")
            except Exception as e:
                print(f"[Data] ‚ö†Ô∏è Could not process timestamp: {e}")
                X = X.drop('timestamp', axis=1)

        # Convert IP addresses to numeric features if they exist
        for ip_col in ['srcip', 'dstip']:
            if ip_col in X.columns and ip_col in categorical_features:
                try:
                    # Extract the last octet as a numeric feature
                    X[f"{ip_col}_last_octet"] = X[ip_col].apply(
                        lambda x: int(str(x).split('.')[-1]) if isinstance(x, str) and '.' in str(x) else 0
                    )
                    numeric_features.append(f"{ip_col}_last_octet")
                except Exception as e:
                    print(f"[Data] ‚ö†Ô∏è Could not process {ip_col}: {e}")

        print(f"[Data] ‚úÖ Data loading complete")
        return X, y, numeric_features, categorical_features
    except Exception as e:
        print(f"[Data] ‚ùå Failed to load {filepath}: {e}")
        # Generate emergency synthetic data
        print(f"[Data] üîÑ Generating emergency synthetic dataset")
        from sklearn.datasets import make_classification
        X, y = make_classification(n_samples=1000, n_features=10, random_state=42)
        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
        categorical_features = []
        numeric_features = [f'feature_{i}' for i in range(10)]
        return df, y, numeric_features, categorical_features

def preprocess_data(X, numeric_features, categorical_features, n_components=0.95):
    print(f"[Preprocess] ‚öôÔ∏è Creating preprocessing pipeline")
    
    # Create transformer for numeric features
    numeric_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', RobustScaler()),
        ('maxabs', MaxAbsScaler())
    ])
    
    # Create the column transformer components
    transformers = [
        ('num', numeric_transformer, numeric_features)
    ]
    
    # Add transformer for categorical features if they exist
    if categorical_features:
        categorical_transformer = Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        transformers.append(('cat', categorical_transformer, categorical_features))
        print(f"[Preprocess] ‚ÑπÔ∏è Using {len(categorical_features)} categorical features for one-hot encoding")
    else:
        print(f"[Preprocess] ‚ÑπÔ∏è No categorical features found for encoding")

    # Create the ColumnTransformer
    preprocessor = ColumnTransformer(transformers)
    
    # Determine the number of components for PCA
    print(f"[Preprocess] üîç Creating pipeline with PCA(n_components={n_components})")
    
    # Create the pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('pca', PCA(n_components=n_components))
    ])

    # Fit and transform the data
    print(f"[Preprocess] üîÑ Fitting and transforming data with shape {X.shape}")
    X_processed = pipeline.fit_transform(X)
    print(f"[Preprocess] ‚úÖ Data processed. New shape: {X_processed.shape}")
    
    return X_processed, pipeline


def select_features(X, y, feature_names, k=20):
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    feature_names = np.array(feature_names)
    selected_features = feature_names[selector.get_support()]
    return X_selected, selected_features


def train_supervised_model(X_train, y_train):
    print(f"[Training] üß† Training Random Forest Classifier")
    print(f"[Training] üîç Performing grid search for hyperparameter optimization")
    
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'class_weight': ['balanced', None]
    }
    
    # Calculate total number of models to try
    total_combinations = (len(param_grid['n_estimators']) * 
                        len(param_grid['max_depth']) * 
                        len(param_grid['min_samples_split']) * 
                        len(param_grid['class_weight']))
    
    print(f"[Training] ‚ÑπÔ∏è Testing {total_combinations} hyperparameter combinations with 3-fold cross-validation")
    print(f"[Training] ‚ÑπÔ∏è Total: {total_combinations * 3} model fits")

    class ProgressCallback:
        def __init__(self, total_iters):
            self.total_iters = total_iters
            self.current_iter = 0
            self.start_time = time.time()
            self.last_update_time = time.time()
            
        def __call__(self, *args, **kwargs):
            self.current_iter += 1
            current_time = time.time()
            
            # Update progress bar every 1 second or at completion
            if (current_time - self.last_update_time > 1) or (self.current_iter >= self.total_iters):
                self.last_update_time = current_time
                elapsed = current_time - self.start_time
                progress = min(100, int(100 * self.current_iter / self.total_iters))
                bars = "=" * (progress // 10) + " " * (10 - progress // 10)
                
                if progress > 0:
                    estimated_total = elapsed * self.total_iters / self.current_iter
                    remaining = estimated_total - elapsed
                    sys.stdout.write(f"\r[Training] Progress: [{bars}] {progress}% | " + 
                                    f"Model {self.current_iter}/{self.total_iters} | " +
                                    f"Time: {elapsed:.1f}s | Remaining: {remaining:.1f}s")
                else:
                    sys.stdout.write(f"\r[Training] Progress: [{bars}] {progress}% | Starting...")
                sys.stdout.flush()
    
    progress_callback = ProgressCallback(total_combinations * 3)  # 3 folds
    
    # Define the classifier
    clf = RandomForestClassifier(random_state=42)
    
    # Setup grid search with callback
    grid_search = GridSearchCV(
        estimator=clf,
        param_grid=param_grid,
        cv=StratifiedKFold(n_splits=3),
        n_jobs=-1,
        verbose=0,  # Disable built-in verbosity
        scoring=make_scorer(custom_scorer),
        error_score='raise'
    )
    
    # Wrap fit to show progress
    old_fit = grid_search.fit
    def fit_with_callback(X, y):
        old_fit(X, y)
        progress_callback(None)  # Final call to ensure 100%
        sys.stdout.write("\n")  # New line after progress completion
        return grid_search
    
    grid_search.fit = fit_with_callback
    
    # Train with progress tracking
    grid_search = grid_search.fit(X_train, y_train)
    
    # Print results
    print(f"[Training] ‚úÖ Grid search complete")
    print(f"[Training] üèÜ Best parameters: {grid_search.best_params_}")
    print(f"[Training] üìä Best score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

def train_anomaly_detector(X_train, contamination='auto'):
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    iso_forest.fit(X_train)
    return iso_forest

def find_optimal_threshold(model, X_val, y_val):
    probs = model.predict_proba(X_val)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(y_val, probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    optimal_threshold = thresholds[np.argmax(f1_scores)]
    return optimal_threshold

def evaluate_model(y_true, y_pred, X_test, model, feature_names, log_file, sample_rate=0.1):
    cv = StratifiedKFold(n_splits=5)
    cv_scores = cross_val_score(model, X_test, y_true, cv=cv, scoring='f1')
    print(f"Cross-validation F1 scores: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    f1 = f1_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_pred)
    mcc = matthews_corrcoef(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)

    print("\nConfusion Matrix:")
    print(cm)

    suspicious_indices = np.where(y_pred == 1)[0]
    if len(suspicious_indices) > 0:
        sample_size = max(1, int(len(suspicious_indices) * sample_rate))
        sampled_indices = np.random.choice(suspicious_indices, size=sample_size, replace=False)
        for index in sampled_indices:
            log_entry = X_test.iloc[index].to_dict()
            log_entry['true_label'] = y_true.iloc[index]
            log_entry['predicted_label'] = y_pred[index]
            logging.info(f"Suspicious log detected: {log_entry}")

    return {
        'F-score': f1,
        'Precision': precision,
        'Recall': recall,
        'AUC-ROC': auc,
        'MCC': mcc
    }

def feature_interaction(X, feature_names, top_features=5):
    top_feature_names = feature_names[:top_features]
    interactions = []
    for feat1, feat2 in combinations(top_feature_names, 2):
        interaction_name = f"{feat1}_{feat2}_interaction"
        interaction_value = X[feat1] * X[feat2]
        interactions.append((interaction_name, interaction_value))

    interactions_df = pd.DataFrame(dict(interactions), index=X.index)
    return interactions_df

def save_model(model_dict, model_dir="../models"):
    """Save trained model and associated metadata to disk."""
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = os.path.join(model_dir, f"log_analyzer_model_{timestamp}")
    os.makedirs(model_path, exist_ok=True)
    
    # Save models
    joblib.dump(model_dict['supervised_model'], os.path.join(model_path, "supervised_model.pkl"))
    joblib.dump(model_dict['anomaly_detector'], os.path.join(model_path, "anomaly_detector.pkl"))
    joblib.dump(model_dict['preprocessor'], os.path.join(model_path, "preprocessor.pkl"))
    
    # Save metadata
    metadata = {
        'numeric_features': list(model_dict['numeric_features']),
        'categorical_features': list(model_dict['categorical_features']),
        'selected_features': list(model_dict['selected_features']),
        'optimal_threshold': model_dict['optimal_threshold'],
        'training_metrics': model_dict['metrics'],
        'timestamp': timestamp
    }
    
    with open(os.path.join(model_path, "metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=4)
    
    print(f"Model saved successfully to {model_path}")
    return model_path

def load_model(model_path):
    """Load trained model and associated metadata from disk."""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model path {model_path} does not exist")
    
    # Load models
    supervised_model = joblib.load(os.path.join(model_path, "supervised_model.pkl"))
    anomaly_detector = joblib.load(os.path.join(model_path, "anomaly_detector.pkl"))
    preprocessor = joblib.load(os.path.join(model_path, "preprocessor.pkl"))
    
    # Load metadata
    with open(os.path.join(model_path, "metadata.json"), 'r') as f:
        metadata = json.load(f)
    
    model_dict = {
        'supervised_model': supervised_model,
        'anomaly_detector': anomaly_detector,
        'preprocessor': preprocessor,
        'numeric_features': metadata['numeric_features'],
        'categorical_features': metadata['categorical_features'],
        'selected_features': metadata['selected_features'],
        'optimal_threshold': metadata['optimal_threshold'],
        'metrics': metadata['training_metrics']
    }
    
    print(f"Model loaded successfully from {model_path}")
    return model_dict

def predict_log_instance(log_instance, model_dict):
    """
    Process a single log instance and return prediction.
    
    Args:
        log_instance: DataFrame or dict containing a single log entry
        model_dict: Dictionary containing the trained models and metadata
    
    Returns:
        Dict containing prediction results and confidence score
    """
    if isinstance(log_instance, dict):
        log_instance = pd.DataFrame([log_instance])
    
    # Ensure log instance has required features
    for feature in model_dict['numeric_features'] + model_dict['categorical_features']:
        if feature not in log_instance.columns:
            if feature in ['hour', 'day_of_week'] and 'timestamp' in log_instance.columns:
                # Handle timestamp conversion
                log_instance['timestamp'] = pd.to_datetime(log_instance['timestamp'], errors='coerce')
                log_instance['hour'] = log_instance['timestamp'].dt.hour
                log_instance['day_of_week'] = log_instance['timestamp'].dt.dayofweek
                log_instance = log_instance.drop('timestamp', axis=1)
            else:
                log_instance[feature] = np.nan  # Fill missing features with NaN
    
    # Preprocess the log instance
    log_processed = model_dict['preprocessor'].transform(log_instance)
    
    # Convert to DataFrame with PCA feature names
    n_components = log_processed.shape[1]
    pca_feature_names = [f"PC{i+1}" for i in range(n_components)]
    log_df = pd.DataFrame(log_processed, columns=pca_feature_names)
    
    # Select features
    log_selected = log_df[model_dict['selected_features']]
    
    # Get predictions
    supervised_prob = model_dict['supervised_model'].predict_proba(log_selected)[0, 1]
    supervised_pred = 1 if supervised_prob >= model_dict['optimal_threshold'] else 0
    
    anomaly_pred = model_dict['anomaly_detector'].predict(log_selected)[0]
    anomaly_pred = 1 if anomaly_pred == -1 else 0
    
    # Final prediction (anomalous if either model predicts anomalous)
    final_pred = 1 if supervised_pred == 1 or anomaly_pred == 1 else 0
    
    result = {
        'prediction': 'Anomalous' if final_pred == 1 else 'Normal',
        'confidence': supervised_prob,
        'supervised_model_prediction': supervised_pred,
        'anomaly_detector_prediction': anomaly_pred,
        'timestamp': datetime.datetime.now().isoformat()
    }
    
    return result

def train_model(train_filepath, test_filepath=None):
    """Train the model and return trained model components."""
    log_file = setup_logging()
    
    try:
        print("\nüöÄ Starting model training process...")
        print("=" * 50)
        start_time = time.time()
        
        # Step 1: Load Data
        print("\nüìä [Step 1/7] Loading training data")
        X_train, y_train, numeric_features, categorical_features = load_unsw_nb15(train_filepath)
        print(f"‚úÖ Loaded {X_train.shape[0]} samples with {X_train.shape[1]} features")
        print(f"   - {sum(y_train)} anomalous samples ({sum(y_train)/len(y_train)*100:.1f}%)")
        print(f"   - {len(y_train) - sum(y_train)} normal samples ({(1-sum(y_train)/len(y_train))*100:.1f}%)")
        
        # Step 2: Preprocess Data
        print("\nüîÑ [Step 2/7] Preprocessing data")
        X_train_processed, preprocessor = preprocess_data(
            X_train, numeric_features, categorical_features, n_components=0.95
        )
        n_components = X_train_processed.shape[1]
        print(f"‚úÖ Data preprocessed, reduced to {n_components} principal components")
        
        pca_feature_names = [f"PC{i+1}" for i in range(n_components)]
        X_train_df = pd.DataFrame(X_train_processed, columns=pca_feature_names, index=X_train.index)
        
        # Step 3: Feature Selection
        print("\nüîç [Step 3/7] Selecting most important features")
        X_train_selected, selected_features = select_features(
            X_train_df, y_train, pca_feature_names, k=min(20, n_components)
        )
        print(f"‚úÖ Selected {len(selected_features)} features from {n_components} components")
        
        # Step 4: Train Supervised Model
        print("\nüß† [Step 4/7] Training supervised model (Random Forest)")
        print("    This may take a few minutes as we're optimizing hyperparameters...")
        supervised_model = train_supervised_model(X_train_selected, y_train)
        print(f"‚úÖ Supervised model training complete")
        
        # Step 5: Train Anomaly Detector
        print("\nüîé [Step 5/7] Training anomaly detector (Isolation Forest)")
        anomaly_detector = train_anomaly_detector(
            X_train_selected[y_train == 0], contamination='auto'
        )
        print(f"‚úÖ Anomaly detector training complete")
        
        # Step 6: Find Optimal Threshold
        print("\n‚öñÔ∏è [Step 6/7] Finding optimal threshold for classification")
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train_selected, y_train, test_size=0.2, stratify=y_train, random_state=42
        )
        supervised_model.fit(X_train_split, y_train_split)
        optimal_threshold = find_optimal_threshold(supervised_model, X_val, y_val)
        print(f"‚úÖ Optimal threshold: {optimal_threshold:.4f}")
        
        # Step 7: Evaluate on test set if provided
        metrics = {}
        if test_filepath:
            print("\nüìè [Step 7/7] Evaluating model on test data")
            X_test, y_test, _, _ = load_unsw_nb15(test_filepath)
            print(f"   Loading {X_test.shape[0]} test samples")
            
            print("   Preprocessing test data...")
            X_test_processed = preprocessor.transform(X_test)
            X_test_df = pd.DataFrame(X_test_processed, columns=pca_feature_names, index=X_test.index)
            X_test_selected = X_test_df[selected_features]
            
            print("   Generating predictions...")
            probs = supervised_model.predict_proba(X_test_selected)[:, 1]
            supervised_preds = (probs >= optimal_threshold).astype(int)
            
            anomaly_preds = anomaly_detector.predict(X_test_selected)
            anomaly_preds = np.where(anomaly_preds == -1, 1, 0)
            
            final_preds = np.where(supervised_preds == 1, 1, anomaly_preds)
            
            print("   Computing evaluation metrics...")
            metrics = evaluate_model(
                y_test, final_preds, X_test_df, supervised_model, selected_features, log_file, sample_rate=0.1
            )
            print("\nüìä Model Evaluation Results:")
            for metric, value in metrics.items():
                print(f"   - {metric}: {value:.4f}")
            
            # Feature importance
            importances = supervised_model.feature_importances_
            feature_importance = pd.DataFrame({
                'feature': selected_features,
                'importance': importances
            })
            feature_importance = feature_importance.sort_values('importance', ascending=False)
            print("\nüîù Top 5 Feature Importances:")
            for i, (feature, importance) in enumerate(zip(feature_importance['feature'].head(5), 
                                                    feature_importance['importance'].head(5))):
                print(f"   {i+1}. {feature}: {importance:.4f}")
        else:
            print("\n‚è© [Step 7/7] Skipping evaluation (no test data provided)")
        
        training_time = time.time() - start_time
        print("\n=" * 50)
        print(f"‚úÖ Model training completed in {training_time:.2f} seconds ({training_time/60:.1f} minutes)")
        
        # Create model dictionary
        model_dict = {
            'supervised_model': supervised_model,
            'anomaly_detector': anomaly_detector,
            'preprocessor': preprocessor,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'selected_features': selected_features,
            'optimal_threshold': optimal_threshold,
            'metrics': metrics
        }
        
        return model_dict
        
    except Exception as e:
        print(f"\n‚ùå An error occurred during training: {e}")
        logging.error(f"An error occurred during training: {e}")
        return None 